import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(
            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion *
                               planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion*planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out
    
    def zero_grad(self):
        """Sets gradients of all model parameters to zero."""
        for p in self.parameters():
            if p.grad is not None:
                p.grad.data.zero_()


def ResNet18():
    return ResNet(BasicBlock, [2, 2, 2, 2])


def ResNet34():
    return ResNet(BasicBlock, [3, 4, 6, 3])

 
def ResNet50():
    return ResNet(Bottleneck, [3, 4, 6, 3])


def ResNet101():
    return ResNet(Bottleneck, [3, 4, 23, 3])


def ResNet152():
    return ResNet(Bottleneck, [3, 8, 36, 3])


losses = []
diversities = []
entropies = []

def getError(modelList, x, y, criterion, alpha=2, gamma=0.5):
  predictions = []
  ensemblePrediction = None
  bestPrediction = None
  bestLoss = None
  totalLoss = None

  index = y[0].item()
  for i in range(len(modelList)):
    model = modelList[i]
    yhat = model(x)
    nolabelPrediction = torch.cat([yhat[0][:index], yhat[0][index+1:]])
    normalizeOp = torch.nn.Softmax()
    normalizedPrediction = normalizeOp(yhat)
    loss = criterion(yhat, y)
    if ensemblePrediction is None: 
      ensemblePrediction = normalizedPrediction
      totalLoss = loss
      bestLoss = loss
      bestPrediction = i
    else:
      ensemblePrediction = ensemblePrediction + normalizedPrediction
      totalLoss = totalLoss + loss
      if bestLoss.item() > loss.item():
        bestLoss = loss
        bestPrediction = i
    nolabelPrediction = nolabelPrediction / (nolabelPrediction**2).sum()**0.5
    predictions.append(nolabelPrediction.view(1, -1))

  """calculating volume spanned by nonmaximal predictions"""
  del predictions[bestPrediction]
  nonmaximalPredictions = torch.cat(predictions)
  matrix = torch.mm(nonmaximalPredictions, torch.transpose(nonmaximalPredictions, 0, 1))
  gramDeterminant = torch.det(matrix)
  diversities.append(gramDeterminant)

  """calculating entropy of ensemble"""
  ensemblePrediction /= len(modelList)
  logValues = torch.log2(ensemblePrediction)
  entropy = -1 * torch.dot(ensemblePrediction[0], logValues[0])
  entropies.append(entropy)

  compositeError = totalLoss - entropy * alpha - torch.log2(gramDeterminant) * gamma
  return compositeError  


import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=2)

"""dropout -> autoencoder -> dropout -> {encoder part of first autoencoder} -> classifier"""
"""autoencoder in a manner is trained to maintain its information, even subject to enormously destructive perturbations"""
"""desturction of any useful gradient information by applying aggressive dropout in combination with regeneration of lost information"""
"""trained in tandom with another autoencoder on non-perturbed images...trained to recognize robust feauteres"""

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn

# functions to show an image


def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# get some random training images
dataiter = iter(trainloader)
images, labels = dataiter.next()

# show images
imshow(torchvision.utils.make_grid(images))
# print labels
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))

import torch.optim as optim

ensemble = []
optimizers = []
for i in range(5):
  ensemble.append(ResNet18())
  optimizers.append(optim.Adam(ensemble[i].parameters(), lr=0.01))

criterion = nn.CrossEntropyLoss()

for epoch in range(1):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        loss = 0
        for j in range(len(data)):
          inputImage = torch.ones(1, 3, 32, 32)
          inputImage[0] = data[0][j]  
          label = torch.tensor([data[1][j].item()])

          for optimizer in optimizers: optimizer.zero_grad()
          
          loss += getError(ensemble, inputImage, label, criterion)

        loss.backward()

        for optimizer in optimizers: optimizer.step()
        print(i)
        losses.append(loss)
        if i == 100: break

print('Finished Training')

import torch
from torch import nn
from torch.autograd import Variable


embeddingSize = 100
ndf = ngf = 32 # controls the number of convolution channels
nc = 3 # number of image channels (3 for color)

encoder = nn.Sequential(
      nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ndf),
      nn.LeakyReLU(0.2, inplace=True),
      nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ndf * 2),
      nn.LeakyReLU(0.2, inplace=True),
      nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ndf * 4),
      nn.LeakyReLU(0.2, inplace=True),
      nn.Conv2d(ndf * 4, embeddingSize, 4, 1, 0, bias=False),
      nn.ReLU()
    )

decoder = nn.Sequential(
      nn.ConvTranspose2d(embeddingSize, ngf * 4, 4, 1, 0, bias=False),
      nn.BatchNorm2d(ngf * 4),
      nn.ReLU(True),
      nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ngf * 2),
      nn.ReLU(True),
      nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ngf),
      nn.ReLU(True),
      nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
      nn.Tanh()
    )


# example
x = Variable(torch.randn(100, nc, 32, 32))
embedding = encoder(x).view(-1, embeddingSize, 1, 1)
reconstruction = decoder(embedding)

plt.plot(losses)
plt.ylabel('losses')
plt.show()

plt.plot(diversities)
plt.ylabel('diversity')
plt.show()

plt.plot(entropies)
plt.ylabel('entropy')
plt.show()

"""
def test():
  net = ResNet50()
  net = net.float()

  inputImage = torch.autograd.Variable(torch.from_numpy(np.ones([1,3,32,32])), requires_grad=True)
  clonedInputImage = inputImage.clone()
  clonedInputImage.retain_grad()

  result = net(clonedInputImage.float())
  v = torch.autograd.Variable(torch.from_numpy(np.ones([1,10])))
  loss = ((v - result)**2).sum()
  loss.backward()
  print(clonedInputImage.grad)
  
  result = net(inputImage.float())
  loss = ((v - result)**2).sum()
  totalLoss = loss + (clonedInputImage.grad**2).sum()
  totalLoss.backward()
  print(inputImage.grad)
"""
